# 7. 熵是为消除不确定性所需要获得的信息量，投掷均匀正六面体骰子的熵是

<iframe src="https://ghbtns.com/github-btn.html?user=geekcircle&repo=machine-learning-interview-qa&type=star&count=true&size=large" frameborder="0" scrolling="0" width="160px" height="30px"></iframe>

2.6比特


## 信息熵


如何计算信息量的多少？在日常生活中，极少发生的事件一旦发生是容易引起人们关注的，而司空见惯的事不会引起注意，也就是说，极少见的事件所带来的信息量多。如果用统计学的术语来描述，就是出现概率小的事件信息量多。因此，事件出现得概率越小，信息量愈大。即信息量的多少是与事件发生频繁(即概率大小)成反比。  

　　1.如已知事件Xi已发生，则表示Xi所含有或所提供的信息量  


H(Xi) = − logP(Xi)

　　例题：若估计在一次国际象棋比赛中谢军获得冠军的可能性为0.1(记为事件A)，而在另一次国际象棋比赛中她得到冠军的可能性为0.9(记为事件B)。试分别计算当你得知她获得冠军时，从这两个事件中获得的信息量各为多少？

　　　　　　H(A)=-log2 P(0.1)≈3.32(比特)

　　　　　　H(B)=-log2 P(0.9)≈0.152(比特)

　　2.统计信息量的计算公式为：
H(X)=$$sum^{n}_{i=1}P(Xi)log P(Xi)$$

Xi —— 表示第i个状态(总共有n种状态)；

P(Xi)——表示第i个状态出现的概率；

H(X)——表示用以消除这个事物的不确定性所需要的信息量。

　　例题：向空中投掷硬币，落地后有两种可能的状态，一个是正面朝上，另一个是反面朝上，每个状态出现的概率为1/2。如投掷均匀的正六面体的骰子，则可能会出现的状态有6个，每一个状态出现的概率均为1/6。试通过计算来比较状态的不肯定性与硬币状态的不肯定性的大小。

H(硬币)= -\sum^{n}_{i=1}P(Xi)log P(Xi)

= -(2×1/2)×logP2(1/2)≈1(比特)

H(骰子)= -\sum^{n}_{i=1}P(Xi)log P(Xi)

= -6×(1/6)×logP2(1/6)≈2.6(比特)

　　由以上计算可以得出两个推论：

　　[推论1] 当且仅当某个P(Xi)=1，其余的都等于0时， H(X)= 0。

　　[推论2]当且仅当某个P(Xi)=1/n，i=1， 2，……， n时，H(X)有极大值log n。
数值属性是可度量的量，用整数或实数值表示，有区间标度和比率标度两种类型。





